---
id: ch17-llm-robots
title: Chapter 17 - Large Language Models for Robots
description: Integrate LLMs for natural language understanding, task planning, and code generation.
sidebar_label: Ch 17. LLMs for Robots
sidebar_position: 3
keywords: [large language models, LLM, GPT, natural language, task planning]
difficulty: advanced
estimatedReadingTime: 35
---

# Chapter 17: Large Language Models for Robots

## Learning Objectives

By the end of this chapter, you will be able to:
- Integrate LLMs with robotic systems
- Use LLMs for natural language understanding
- Implement LLM-based task planning
- Generate robot code using LLMs

## Introduction

Large Language Models (LLMs) are transforming robotics by enabling natural language interaction, high-level task planning, and even code generation. By grounding language in physical actions, robots can understand human instructions, reason about tasks, and adapt to new situations with unprecedented flexibility.

## Why LLMs for Robotics?

### Traditional Limitations

**Pre-LLM Robot Programming:**
- Fixed command sets ("move forward", "turn left")
- Brittle to instruction variations
- No common-sense reasoning
- Difficult to adapt to new tasks

**LLM Advantages:**
- Natural language understanding ("Can you hand me the blue mug?")
- Contextual reasoning ("The door is closed, so open it first")
- Zero-shot generalization to novel tasks
- Human-like interaction

### Key Capabilities

1. **Natural Language Understanding**: Parse complex, ambiguous human instructions
2. **Task Planning**: Decompose high-level goals into executable action sequences
3. **Code Generation**: Synthesize robot control code from descriptions
4. **Contextual Reasoning**: Infer implicit constraints and preferences
5. **Affordance Reasoning**: Understand object properties and possible interactions

## LLM Architecture for Robotics

### Standard LLM Pipeline

**1. Input Processing**
```
Human: "Pick up the red block and place it on the table"
↓
Tokenization: [Pick, up, the, red, block, and, place, it, on, the, table]
↓
Embeddings: Convert tokens to numerical vectors
```

**2. Transformer Processing**
- Self-attention mechanisms capture token relationships
- Multiple layers extract hierarchical representations
- Context length: 4K-128K tokens (depending on model)

**3. Output Generation**
```
LLM Output:
1. Identify red block using computer vision
2. Move gripper to block location
3. Close gripper to grasp block
4. Lift block
5. Move to table surface
6. Release gripper
```

### Grounding LLMs in Physical World

**Challenge**: LLMs are trained on text, not physical interactions.

**Solution**: Multimodal grounding

**Vision-Language Models (VLMs):**
- CLIP: Aligns images and text in shared embedding space
- Flamingo: Integrates visual inputs with language processing
- GPT-4V, Gemini Vision: Native image understanding

**Sensory Integration:**
```
Input: [Image, Depth Map, Force Sensor Data, Natural Language Command]
        ↓
      VLM
        ↓
   Action Plan
```

## LLM-Based Task Planning

### Hierarchical Task Decomposition

**High-Level Goal**: "Make me breakfast"

**LLM Decomposition**:
```
1. Determine breakfast items (query user preferences)
2. Locate ingredients (eggs, bread, butter)
3. Prepare toast:
   a. Place bread in toaster
   b. Press start button
   c. Wait for completion
4. Prepare scrambled eggs:
   a. Crack eggs into bowl
   b. Whisk eggs
   c. Heat pan
   d. Pour eggs into pan
   e. Stir until cooked
5. Plate food and serve
```

### Planning with Constraints

**Example**: "Move the laptop to the desk, but avoid spilling the coffee cup"

**LLM Reasoning**:
```python
Constraints identified:
- Primary goal: Transport laptop
- Safety constraint: Maintain distance from coffee cup
- Spatial reasoning: Plan trajectory avoiding coffee cup location

Generated plan:
1. Visually locate laptop and coffee cup
2. Compute safe path (> 20cm clearance from cup)
3. Grasp laptop with both manipulators for stability
4. Execute trajectory with collision avoidance active
5. Place laptop on desk surface
```

### Example: SayCan (Google)

**Architecture**:
```
User: "I spilled my drink, can you help?"
    ↓
LLM (PaLM): Generates candidate plans
    ↓
Value Function: Scores plans by feasibility (learned from robot experience)
    ↓
Selected Plan:
1. Find sponge
2. Pick up sponge
3. Navigate to spill
4. Wipe area
5. Return sponge
```

**Key Insight**: Combine LLM's language understanding with learned affordance models.

## Code Generation for Robots

### Prompt Engineering

**Effective Prompts Include:**
1. Robot capabilities and API documentation
2. Environment description
3. Safety constraints
4. Example code snippets

**Example Prompt**:
```
# Robot API Documentation
robot.move_to(x, y, z)  # Move end-effector to position
robot.grasp()           # Close gripper
robot.release()         # Open gripper
robot.get_object_pose(object_name) # Returns (x, y, z)

# Environment
- Objects: red_block, blue_block, table
- Current gripper state: open

# Task
Generate Python code to stack the red block on top of the blue block.

# Safety Constraints
- Always check grasp success before lifting
- Maintain 5cm/s max velocity near objects
```

**LLM Generated Code**:
```python
# Get object locations
red_pos = robot.get_object_pose("red_block")
blue_pos = robot.get_object_pose("blue_block")

# Pick up red block
robot.move_to(red_pos[0], red_pos[1], red_pos[2] + 0.1)  # Approach from above
robot.move_to(red_pos[0], red_pos[1], red_pos[2])         # Descend
robot.grasp()
if robot.is_grasping():  # Verify grasp success
    robot.move_to(red_pos[0], red_pos[1], red_pos[2] + 0.15)  # Lift

    # Place on blue block
    target_x, target_y, target_z = blue_pos
    robot.move_to(target_x, target_y, target_z + 0.12, velocity=0.05)  # Slow approach
    robot.release()
else:
    print("Grasp failed, aborting task")
```

### Code-as-Policies (CaP)

**Concept**: LLMs generate executable policies in code form, enabling:
- Precise numerical computations (vs. text descriptions)
- Integration with external libraries (vision, planning)
- Modularity and reusability

**Example (Google Research)**:
```python
# Generated by LLM
def put_block_in_box(robot, block_color, box_name):
    """Place colored block into specified box."""
    block_pose = vision.detect_object(f"{block_color}_block")
    box_pose = vision.detect_object(box_name)

    # Pick block
    robot.pick(block_pose)

    # Place in box (compute position above box center)
    target = (box_pose.x, box_pose.y, box_pose.z + 0.1)
    robot.place(target)
```

## Natural Language Interfaces

### Conversational Interaction

**Multi-Turn Dialog**:
```
Human: "Can you hand me the wrench?"
Robot: "I see two wrenches. Do you mean the adjustable wrench or the socket wrench?"
Human: "The adjustable one."
Robot: "Got it. Picking up the adjustable wrench now."
[Executes pick and handover]
Robot: "Here you go."
```

**Context Maintenance**: LLMs track conversation history to resolve ambiguous references.

### Clarification and Error Recovery

**Handling Ambiguity**:
```
Human: "Move the block to the left."
Robot: "I see three blocks. Which block should I move?"
Human: "The red one."
Robot: "Should I move it to the left of its current position, or to the leftmost area of the table?"
Human: "Just a bit to the left, about 10 cm."
Robot: "Moving the red block 10 cm to the left."
```

**Failure Explanation**:
```
Human: "Grasp the metal rod."
Robot: "I attempted to grasp the metal rod, but my gripper failed to secure it. The rod's
diameter (15mm) exceeds my gripper's maximum capacity (10mm). Would you like me to try
a different approach or use a different tool?"
```

## Practical Architectures

### Architecture 1: LLM as High-Level Planner

```
┌─────────────────────────────────────┐
│ User Instruction (Natural Language) │
└──────────────┬──────────────────────┘
               ↓
┌──────────────────────────────────────┐
│  LLM (Task Planner)                  │
│  - Decompose task into subtasks      │
│  - Generate action sequence          │
└──────────────┬───────────────────────┘
               ↓
┌──────────────────────────────────────┐
│  Motion Planner (Traditional)        │
│  - Compute trajectories              │
│  - Collision avoidance               │
└──────────────┬───────────────────────┘
               ↓
┌──────────────────────────────────────┐
│  Robot Controller                    │
│  - Execute motions                   │
│  - Sensor feedback                   │
└──────────────────────────────────────┘
```

### Architecture 2: End-to-End VLA (Vision-Language-Action)

Covered in detail in Chapter 18.

### Architecture 3: LLM with Tool Use

**Concept**: LLM selects and invokes specialized tools/functions

**Example Tools**:
- `get_object_pose(name)`: Vision-based object localization
- `plan_trajectory(start, goal)`: Motion planning
- `execute_grasp(object_id)`: Manipulation primitive
- `check_collision(path)`: Safety verification

**LLM Reasoning**:
```
User: "Pick up the bottle and check if it's empty."

LLM Tool Calls:
1. bottle_pose = get_object_pose("bottle")
2. grasp_result = execute_grasp(bottle_pose)
3. weight = get_gripper_force_reading()
4. if weight < 50g:
     output("The bottle appears empty based on its weight.")
   else:
     output("The bottle is not empty.")
```

## Training and Fine-Tuning

### Pre-trained Base Models

**Popular Models**:
- GPT-4, GPT-3.5 (OpenAI)
- PaLM, Gemini (Google)
- Claude (Anthropic)
- LLaMA (Meta - open source)

**Advantages**: Strong general reasoning, no training required

**Limitations**: May not understand robot-specific APIs or constraints

### Fine-Tuning on Robot Data

**Dataset Requirements**:
- Instruction-action pairs
- Success/failure labels
- Environment descriptions

**Example Entry**:
```json
{
  "instruction": "Move the mug to the shelf",
  "environment": "Kitchen with counter, shelf, sink. Mug on counter.",
  "action_sequence": [
    "detect_object('mug')",
    "grasp_object('mug')",
    "navigate_to('shelf')",
    "place_object('mug', 'shelf')"
  ],
  "success": true
}
```

**Fine-Tuning Methods**:
- **Full fine-tuning**: Update all model parameters (expensive)
- **LoRA**: Low-Rank Adaptation (efficient, 0.1% of parameters)
- **Prompt tuning**: Learn soft prompts (even more efficient)

### Reinforcement Learning from Human Feedback (RLHF)

Align LLM outputs with human preferences for robot safety and effectiveness.

**Process**:
1. Collect human rankings of LLM-generated plans
2. Train reward model to predict human preferences
3. Use PPO or DPO to optimize LLM policy

## Challenges and Limitations

### 1. Grounding Problem

**Issue**: LLMs lack true understanding of physical world

**Example Failure**:
```
Human: "Stack the blocks to reach the ceiling."
LLM: "Stack 100 blocks vertically."
Reality: Only 20 blocks available, and tower would be unstable
```

**Mitigation**: Integrate with physics simulators and feasibility checkers

### 2. Latency

**Problem**: LLM inference can take 1-10 seconds

**Impact**: Unsuitable for real-time reactive control

**Solution**: Use LLMs for high-level planning only, not low-level control

### 3. Safety and Robustness

**Risks**:
- Generated code may contain bugs or unsafe actions
- Adversarial prompts could cause harmful behavior
- No formal verification of outputs

**Mitigation Strategies**:
- Constrain action space to safe primitives
- Sandbox execution environments
- Human-in-the-loop verification for critical tasks
- Confidence thresholding (reject low-confidence outputs)

### 4. Hallucination

**Problem**: LLMs may generate plausible-sounding but incorrect plans

**Example**:
```
Human: "Weld the steel beam."
LLM: "Activate welding torch, apply flux, weld joint."
Reality: Robot doesn't have welding capability
```

**Solution**: Capability-aware prompting and verification against robot API

## Real-World Applications

### Domestic Robots

**Tidying Up**:
```
Human: "Clean up the living room."
LLM Plan:
1. Identify clutter (toys, books, dishes)
2. Categorize items by destination
3. Pick up items one by one
4. Place in appropriate locations (toy box, bookshelf, kitchen)
5. Vacuum floor
```

### Industrial Cobots

**Assembly Assistance**:
```
Worker: "Hand me the M8 bolt."
Robot: "Here's the M8 bolt. Would you also like the corresponding nut and washer?"
Worker: "Yes, thanks."
Robot: [Retrieves and presents parts]
```

### Warehouse Robots

**Dynamic Tasking**:
```
Manager: "Prioritize all orders containing fragile items for the next hour."
LLM: Updates task queue, flags fragile item orders, adjusts picking sequence
```

## Practical Implementation

### Example: OpenAI GPT Integration

```python
import openai
import robot_api

# Initialize
openai.api_key = "your-api-key"
robot = robot_api.RobotController()

def llm_task_planner(user_instruction):
    """Use LLM to generate task plan."""
    prompt = f"""
You are a robot task planner. Generate a step-by-step plan for:
Task: {user_instruction}

Available actions:
- move_to(x, y, z)
- grasp()
- release()
- rotate_joint(joint_name, angle)

Output format: Numbered list of actions with parameters.
"""

    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    )

    plan = response.choices[0].message.content
    return plan

def execute_plan(plan):
    """Parse and execute LLM-generated plan."""
    for line in plan.split('\n'):
        if 'move_to' in line:
            # Parse coordinates and execute
            x, y, z = parse_coordinates(line)
            robot.move_to(x, y, z)
        elif 'grasp' in line:
            robot.grasp()
        # ... handle other actions

# Usage
instruction = "Pick up the red cube"
plan = llm_task_planner(instruction)
print("Generated Plan:", plan)
execute_plan(plan)
```

### Safety Wrapper

```python
def safe_execute(action, params):
    """Verify action safety before execution."""
    # Check workspace bounds
    if action == "move_to":
        x, y, z = params
        if not robot.is_in_workspace(x, y, z):
            return False, "Position outside safe workspace"

    # Check collision
    if robot.collision_check(action, params):
        return False, "Collision detected"

    # Execute if safe
    robot.execute(action, params)
    return True, "Success"
```

## Future Directions

### Embodied Large Language Models

Trained on both language and sensorimotor data from robots:
- Google RT-X: Collected 130K+ robot trajectories across multiple embodiments
- Open X-Embodiment: Multi-institutional dataset for generalist robots

### Multimodal Reasoning

Integrating:
- Vision (object recognition, scene understanding)
- Touch (force feedback, material properties)
- Audio (voice commands, environmental sounds)
- Proprioception (joint states, balance)

### Foundation Models for Robotics

Pre-trained on massive robot interaction data, fine-tuneable for specific robots and tasks.

## Key Takeaways

- LLMs enable natural language interaction and high-level task planning for robots
- Grounding language in physical actions requires multimodal integration (vision, sensors)
- LLMs excel at task decomposition, code generation, and conversational interfaces
- Challenges include latency, safety, grounding, and hallucination
- Practical systems use LLMs for planning, not low-level control
- Fine-tuning and RLHF improve robot-specific performance
- Future: Embodied LLMs trained on robot experience data

## What's Next?

Chapter 18 explores **Vision-Language-Action (VLA) models**, which directly map from visual and language inputs to robot actions, creating end-to-end systems for embodied AI.

### Further Resources

- **Papers**: "Do As I Can, Not As I Say" (SayCan, Google), "Code as Policies" (Google), "RT-2: Vision-Language-Action Models"
- **Frameworks**: LangChain (LLM orchestration), HuggingFace Transformers
- **Datasets**: Open X-Embodiment, Language-Table
- **APIs**: OpenAI API, Anthropic Claude API, Google Gemini API
