---
id: ch15-ai-fundamentals
title: Chapter 15 - AI & ML Fundamentals for Robotics
description: Learn supervised, unsupervised, and neural network foundations for robotics applications.
sidebar_label: Ch 15. AI Fundamentals
sidebar_position: 1
keywords: [machine learning, supervised learning, unsupervised learning, neural networks, PyTorch, robotics AI]
difficulty: intermediate
estimatedReadingTime: 40
---

# Chapter 15: AI & ML Fundamentals for Robotics

## Learning Objectives

By the end of this chapter, you will be able to:
- Understand machine learning paradigms for robotics
- Train neural networks for perception and control tasks
- Apply supervised and unsupervised learning to robot data
- Use PyTorch and TensorFlow for robotics applications
- Evaluate and deploy ML models on robot hardware

## Introduction

Artificial Intelligence and Machine Learning have transformed robotics, enabling robots to learn from experience, adapt to new situations, and handle the complexity of real-world environments. This chapter provides the foundational ML knowledge you need to build intelligent humanoid robots.

Unlike traditional programming where you explicitly code rules, machine learning allows robots to learn patterns from data. This is crucial for robotics because:
- **Real-world complexity**: Too many edge cases to manually program
- **Adaptation**: Robots encounter situations not seen during development
- **Perception**: Extracting meaning from high-dimensional sensor data
- **Control**: Learning complex behaviors through trial and error

### ML in Robotics vs. Traditional ML

**Key Differences:**
- **Real-time constraints**: Inference must be fast (`<100ms` for control)
- **Limited data**: Can't always collect millions of training examples
- **Safety-critical**: Failures can cause physical damage
- **Embodiment**: ML models interact with physical world
- **Continuous learning**: Robots must adapt online

**Common Applications:**
- **Perception**: Object detection, pose estimation, scene understanding
- **Control**: Locomotion, manipulation, whole-body coordination
- **Planning**: Navigation, task planning, decision making
- **Interaction**: Natural language, gesture recognition, social behavior

## Machine Learning Paradigms

### Supervised Learning

**Definition**: Learning from labeled examples (input-output pairs)

**When to Use:**
- You have labeled training data
- Clear input-output relationship
- Classification or regression tasks

**Robotics Applications:**
- Object classification from images
- Grasp quality prediction
- Trajectory optimization
- Failure detection

**Example: Grasp Success Prediction**

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np

class GraspDataset(Dataset):
    """Dataset of grasp attempts with success labels"""

    def __init__(self, features, labels):
        """
        Args:
            features: Grasp parameters (position, orientation, gripper width)
            labels: Success (1) or failure (0)
        """
        self.features = torch.FloatTensor(features)
        self.labels = torch.FloatTensor(labels)

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]

class GraspPredictor(nn.Module):
    """Neural network to predict grasp success"""

    def __init__(self, input_dim=7):
        super(GraspPredictor, self).__init__()

        self.network = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(32, 1),
            nn.Sigmoid()  # Output probability [0, 1]
        )

    def forward(self, x):
        return self.network(x)

# Training loop
def train_grasp_predictor(model, train_loader, val_loader, epochs=50):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)

    criterion = nn.BCELoss()  # Binary cross-entropy
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    for epoch in range(epochs):
        # Training
        model.train()
        train_loss = 0.0

        for features, labels in train_loader:
            features, labels = features.to(device), labels.to(device)

            # Forward pass
            outputs = model(features).squeeze()
            loss = criterion(outputs, labels)

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            train_loss += loss.item()

        # Validation
        model.eval()
        val_loss = 0.0
        correct = 0
        total = 0

        with torch.no_grad():
            for features, labels in val_loader:
                features, labels = features.to(device), labels.to(device)

                outputs = model(features).squeeze()
                loss = criterion(outputs, labels)
                val_loss += loss.item()

                # Accuracy
                predicted = (outputs > 0.5).float()
                correct += (predicted == labels).sum().item()
                total += labels.size(0)

        print(f'Epoch {epoch+1}/{epochs}:')
        print(f'  Train Loss: {train_loss/len(train_loader):.4f}')
        print(f'  Val Loss: {val_loss/len(val_loader):.4f}')
        print(f'  Val Accuracy: {100*correct/total:.2f}%')

    return model

# Usage
# Generate synthetic data (replace with real robot data)
num_samples = 10000
features = np.random.randn(num_samples, 7)  # [x, y, z, roll, pitch, yaw, width]
labels = (np.random.rand(num_samples) > 0.5).astype(float)  # Random success/failure

# Create datasets
dataset = GraspDataset(features, labels)
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32)

# Train model
model = GraspPredictor(input_dim=7)
trained_model = train_grasp_predictor(model, train_loader, val_loader, epochs=50)

# Save model
torch.save(trained_model.state_dict(), 'grasp_predictor.pth')
```

### Unsupervised Learning

**Definition**: Learning patterns from unlabeled data

**When to Use:**
- Unlabeled data is abundant
- Want to discover hidden structure
- Dimensionality reduction needed
- Anomaly detection

**Robotics Applications:**
- Clustering similar objects
- Dimensionality reduction for high-dimensional sensors
- Anomaly detection for fault diagnosis
- Self-supervised pre-training

**Example: Clustering Object Observations**

```python
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import numpy as np
import matplotlib.pyplot as plt

class ObjectClusterer:
    """Cluster objects based on visual features"""

    def __init__(self, n_clusters=5):
        self.n_clusters = n_clusters
        self.kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        self.pca = PCA(n_components=50)  # Dimensionality reduction

    def fit(self, features):
        """
        Learn clusters from object features

        Args:
            features: (N, D) array of feature vectors
        """

        # Reduce dimensionality
        features_reduced = self.pca.fit_transform(features)

        # Cluster
        self.kmeans.fit(features_reduced)

        print(f"Explained variance: {self.pca.explained_variance_ratio_.sum():.2%}")

    def predict(self, features):
        """Assign new objects to clusters"""
        features_reduced = self.pca.transform(features)
        return self.kmeans.predict(features_reduced)

    def visualize_clusters(self, features, labels=None):
        """Visualize clusters in 2D"""

        # Reduce to 2D for visualization
        pca_2d = PCA(n_components=2)
        features_2d = pca_2d.fit_transform(features)

        # Plot
        plt.figure(figsize=(10, 6))

        if labels is None:
            labels = self.predict(features)

        scatter = plt.scatter(features_2d[:, 0], features_2d[:, 1],
                             c=labels, cmap='viridis', alpha=0.6)
        plt.colorbar(scatter, label='Cluster')
        plt.xlabel('PC 1')
        plt.ylabel('PC 2')
        plt.title('Object Clusters')
        plt.show()

# Usage with robot camera data
# Extract features from images (e.g., using ResNet)
def extract_features_from_images(images, model):
    """Extract feature vectors from images using pre-trained CNN"""

    features = []

    model.eval()
    with torch.no_grad():
        for image in images:
            # Preprocess image
            image_tensor = preprocess(image).unsqueeze(0)

            # Forward pass (remove classification layer to get features)
            feature = model.features(image_tensor).flatten().cpu().numpy()
            features.append(feature)

    return np.array(features)

# Cluster objects
clusterer = ObjectClusterer(n_clusters=5)
clusterer.fit(object_features)
clusterer.visualize_clusters(object_features)
```

### Reinforcement Learning (Overview)

**Definition**: Learning through trial and error with rewards

**When to Use:**
- No labeled data, but can measure success
- Agent interacts with environment
- Sequential decision making
- Exploration is safe and feasible

**Robotics Applications:**
- Locomotion (walking, running)
- Manipulation (reaching, grasping)
- Navigation
- Game playing

*Note: Detailed RL covered in Chapter 16*

## Neural Networks for Robotics

### Feedforward Neural Networks

**Architecture:**

```
Input → Hidden Layer 1 → Hidden Layer 2 → Output
  |           ↓                 ↓             ↓
  |        Activation       Activation    Activation
```

**PyTorch Implementation:**

```python
import torch
import torch.nn as nn

class RobotMLPClass RobotMLP(nn.Module):
    """Multi-layer perceptron for robotics tasks"""

    def __init__(self, input_dim, hidden_dims, output_dim, activation='relu'):
        super(RobotMLP, self).__init__()

        # Build layers
        layers = []
        prev_dim = input_dim

        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, hidden_dim))

            if activation == 'relu':
                layers.append(nn.ReLU())
            elif activation == 'tanh':
                layers.append(nn.Tanh())
            elif activation == 'elu':
                layers.append(nn.ELU())

            layers.append(nn.Dropout(0.2))  # Regularization
            prev_dim = hidden_dim

        # Output layer
        layers.append(nn.Linear(prev_dim, output_dim))

        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)

# Example: Joint velocity prediction from sensor data
sensor_dim = 20  # IMU, force sensors, etc.
joint_dim = 12   # 12 joints

model = RobotMLP(
    input_dim=sensor_dim,
    hidden_dims=[128, 64, 32],
    output_dim=joint_dim
)

print(model)
```

### Convolutional Neural Networks (CNNs)

**For Image Processing:**

```python
class RobotVisionCNN(nn.Module):
    """CNN for processing robot camera images"""

    def __init__(self, num_classes=10):
        super(RobotVisionCNN, self).__init__()

        # Convolutional layers
        self.conv_layers = nn.Sequential(
            # Conv block 1
            nn.Conv2d(3, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),  # 224x224 → 112x112

            # Conv block 2
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),  # 112x112 → 56x56

            # Conv block 3
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),  # 56x56 → 28x28

            # Conv block 4
            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),  # 28x28 → 14x14
        )

        # Fully connected layers
        self.fc_layers = nn.Sequential(
            nn.Flatten(),
            nn.Linear(256 * 14 * 14, 512),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(512, num_classes)
        )

    def forward(self, x):
        x = self.conv_layers(x)
        x = self.fc_layers(x)
        return x

# Usage for object classification
model = RobotVisionCNN(num_classes=20)

# Example input: batch of RGB images
batch_size = 16
images = torch.randn(batch_size, 3, 224, 224)  # (B, C, H, W)

# Forward pass
outputs = model(images)  # (B, num_classes)
predictions = torch.argmax(outputs, dim=1)
```

### Recurrent Neural Networks (RNNs)

**For Temporal/Sequential Data:**

```python
class RobotLSTM(nn.Module):
    """LSTM for processing temporal robot data"""

    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(RobotLSTM, self).__init__()

        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # LSTM layer
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=0.2 if num_layers > 1 else 0
        )

        # Output layer
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        """
        Args:
            x: (batch, sequence_length, input_size)

        Returns:
            output: (batch, sequence_length, output_size)
        """

        # LSTM forward pass
        lstm_out, (h_n, c_n) = self.lstm(x)

        # Apply FC to each time step
        output = self.fc(lstm_out)

        return output

# Example: Predict next joint positions from trajectory history
input_size = 12   # Current joint positions
hidden_size = 64
num_layers = 2
output_size = 12  # Next joint positions

model = RobotLSTM(input_size, hidden_size, num_layers, output_size)

# Example input: batch of trajectory sequences
batch_size = 32
seq_length = 50
trajectories = torch.randn(batch_size, seq_length, input_size)

# Predict next positions
predictions = model(trajectories)  # (32, 50, 12)
```

## Training Neural Networks

### Data Preparation

```python
import torch
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms

class RobotDataset(Dataset):
    """Custom dataset for robot data"""

    def __init__(self, data_dir, transform=None):
        self.data_dir = data_dir
        self.transform = transform

        # Load file paths and labels
        self.samples = self.load_samples()

    def load_samples(self):
        # Load your robot data
        # Return list of (data_path, label) tuples
        pass

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        data_path, label = self.samples[idx]

        # Load data (e.g., image, sensor readings)
        data = self.load_data(data_path)

        # Apply transformations
        if self.transform:
            data = self.transform(data)

        return data, label

    def load_data(self, path):
        # Load your data format
        pass

# Data augmentation for images
train_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                        std=[0.229, 0.224, 0.225])
])

val_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                        std=[0.229, 0.224, 0.225])
])

# Create datasets
train_dataset = RobotDataset('data/train', transform=train_transform)
val_dataset = RobotDataset('data/val', transform=val_transform)

# Create data loaders
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)
```

### Training Loop with Best Practices

```python
def train_model(model, train_loader, val_loader, epochs=100, device='cuda'):
    """
    Complete training loop with validation and early stopping
    """

    model.to(device)

    # Loss and optimizer
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)

    # Learning rate scheduler
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=5, verbose=True
    )

    # Early stopping
    best_val_loss = float('inf')
    patience = 10
    patience_counter = 0

    # Training history
    history = {
        'train_loss': [],
        'val_loss': [],
        'train_acc': [],
        'val_acc': []
    }

    for epoch in range(epochs):
        # Training phase
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0

        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            # Forward pass
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            # Backward pass
            optimizer.zero_grad()
            loss.backward()

            # Gradient clipping (prevent exploding gradients)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            optimizer.step()

            # Statistics
            train_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            train_total += labels.size(0)
            train_correct += (predicted == labels).sum().item()

        # Validation phase
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0

        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)

                outputs = model(inputs)
                loss = criterion(outputs, labels)

                val_loss += loss.item()
                _, predicted = torch.max(outputs, 1)
                val_total += labels.size(0)
                val_correct += (predicted == labels).sum().item()

        # Calculate epoch metrics
        train_loss /= len(train_loader)
        val_loss /= len(val_loader)
        train_acc = 100 * train_correct / train_total
        val_acc = 100 * val_correct / val_total

        # Update history
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['train_acc'].append(train_acc)
        history['val_acc'].append(val_acc)

        # Print progress
        print(f'Epoch [{epoch+1}/{epochs}]')
        print(f'  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%')
        print(f'  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%')
        print(f'  LR: {optimizer.param_groups[0]["lr"]:.6f}')

        # Learning rate scheduling
        scheduler.step(val_loss)

        # Early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0

            # Save best model
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': val_loss,
                'val_acc': val_acc
            }, 'best_model.pth')

        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f'Early stopping triggered after {epoch+1} epochs')
                break

    return model, history

# Usage
model = RobotVisionCNN(num_classes=20)
trained_model, history = train_model(model, train_loader, val_loader, epochs=100)
```

### Visualization and Debugging

```python
import matplotlib.pyplot as plt

def plot_training_history(history):
    """Visualize training progress"""

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

    # Loss plot
    ax1.plot(history['train_loss'], label='Train Loss')
    ax1.plot(history['val_loss'], label='Val Loss')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.set_title('Training and Validation Loss')
    ax1.legend()
    ax1.grid(True)

    # Accuracy plot
    ax2.plot(history['train_acc'], label='Train Acc')
    ax2.plot(history['val_acc'], label='Val Acc')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy (%)')
    ax2.set_title('Training and Validation Accuracy')
    ax2.legend()
    ax2.grid(True)

    plt.tight_layout()
    plt.savefig('training_history.png')
    plt.show()

# Visualize
plot_training_history(history)
```

## Model Evaluation

### Metrics for Classification

```python
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import seaborn as sns

def evaluate_classification_model(model, test_loader, device='cuda', class_names=None):
    """Comprehensive evaluation of classification model"""

    model.eval()
    model.to(device)

    all_predictions = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs = inputs.to(device)

            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)

            all_predictions.extend(predicted.cpu().numpy())
            all_labels.extend(labels.numpy())

    # Calculate metrics
    accuracy = accuracy_score(all_labels, all_predictions)
    print(f"Overall Accuracy: {accuracy * 100:.2f}%\n")

    # Classification report
    print("Classification Report:")
    print(classification_report(all_labels, all_predictions,
                                target_names=class_names))

    # Confusion matrix
    cm = confusion_matrix(all_labels, all_predictions)

    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names)
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    plt.savefig('confusion_matrix.png')
    plt.show()

    return accuracy, cm

# Usage
class_names = ['cup', 'bottle', 'box', 'ball', 'tool']
accuracy, cm = evaluate_classification_model(model, test_loader, class_names=class_names)
```

### Metrics for Regression

```python
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

def evaluate_regression_model(model, test_loader, device='cuda'):
    """Evaluate regression model"""

    model.eval()
    model.to(device)

    all_predictions = []
    all_targets = []

    with torch.no_grad():
        for inputs, targets in test_loader:
            inputs = inputs.to(device)

            outputs = model(inputs)

            all_predictions.extend(outputs.cpu().numpy())
            all_targets.extend(targets.numpy())

    all_predictions = np.array(all_predictions)
    all_targets = np.array(all_targets)

    # Calculate metrics
    mae = mean_absolute_error(all_targets, all_predictions)
    mse = mean_squared_error(all_targets, all_predictions)
    rmse = np.sqrt(mse)
    r2 = r2_score(all_targets, all_predictions)

    print(f"Mean Absolute Error: {mae:.4f}")
    print(f"Root Mean Squared Error: {rmse:.4f}")
    print(f"R² Score: {r2:.4f}")

    # Scatter plot
    plt.figure(figsize=(8, 6))
    plt.scatter(all_targets, all_predictions, alpha=0.5)
    plt.plot([all_targets.min(), all_targets.max()],
             [all_targets.min(), all_targets.max()],
             'r--', lw=2)
    plt.xlabel('True Values')
    plt.ylabel('Predictions')
    plt.title('Predictions vs. True Values')
    plt.grid(True)
    plt.savefig('regression_results.png')
    plt.show()

    return mae, rmse, r2
```

## Deploying Models on Robots

### Model Optimization

**1. Model Pruning:**

```python
import torch.nn.utils.prune as prune

def prune_model(model, amount=0.3):
    """Remove less important weights"""

    for name, module in model.named_modules():
        if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):
            prune.l1_unstructured(module, name='weight', amount=amount)
            prune.remove(module, 'weight')  # Make pruning permanent

    return model

# Prune 30% of weights
pruned_model = prune_model(model, amount=0.3)
```

**2. Quantization:**

```python
def quantize_model(model):
    """Convert model to INT8 for faster inference"""

    # Post-training static quantization
    model_quantized = torch.quantization.quantize_dynamic(
        model,
        {nn.Linear, nn.Conv2d},
        dtype=torch.qint8
    )

    return model_quantized

quantized_model = quantize_model(model)

# Compare model sizes
original_size = sum(p.numel() * p.element_size() for p in model.parameters()) / 1024 / 1024
quantized_size = sum(p.numel() * p.element_size() for p in quantized_model.parameters()) / 1024 / 1024

print(f"Original model: {original_size:.2f} MB")
print(f"Quantized model: {quantized_size:.2f} MB")
print(f"Compression ratio: {original_size / quantized_size:.2f}x")
```

**3. ONNX Export:**

```python
def export_to_onnx(model, input_shape, filename='model.onnx'):
    """Export model to ONNX format for deployment"""

    model.eval()

    # Create dummy input
    dummy_input = torch.randn(*input_shape)

    # Export
    torch.onnx.export(
        model,
        dummy_input,
        filename,
        export_params=True,
        opset_version=11,
        do_constant_folding=True,
        input_names=['input'],
        output_names=['output'],
        dynamic_axes={
            'input': {0: 'batch_size'},
            'output': {0: 'batch_size'}
        }
    )

    print(f"Model exported to {filename}")

# Export
export_to_onnx(model, input_shape=(1, 3, 224, 224), filename='robot_vision.onnx')
```

### ROS 2 Integration

```python
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from std_msgs.msg import String
from cv_bridge import CvBridge
import torch
import torchvision.transforms as transforms

class MLInferenceNode(Node):
    """ROS 2 node for ML model inference"""

    def __init__(self):
        super().__init__('ml_inference_node')

        # Load model
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = RobotVisionCNN(num_classes=20)
        self.model.load_state_dict(torch.load('best_model.pth')['model_state_dict'])
        self.model.to(self.device)
        self.model.eval()

        # Image preprocessing
        self.transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                std=[0.229, 0.224, 0.225])
        ])

        self.bridge = CvBridge()

        # Class names
        self.class_names = ['cup', 'bottle', 'box', 'ball', 'tool',
                           'phone', 'book', 'pen', 'scissors', 'tape',
                           'stapler', 'marker', 'eraser', 'clip', 'folder',
                           'calculator', 'notebook', 'mug', 'keyboard', 'mouse']

        # ROS interface
        self.image_sub = self.create_subscription(
            Image, '/camera/image_raw', self.infer_callback, 10
        )

        self.result_pub = self.create_publisher(String, '/ml/classification', 10)

        self.get_logger().info('ML Inference node started')

    def infer_callback(self, msg):
        """Run inference on incoming images"""

        # Convert ROS image to OpenCV
        cv_image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')

        # Preprocess
        input_tensor = self.transform(cv_image).unsqueeze(0).to(self.device)

        # Inference
        with torch.no_grad():
            output = self.model(input_tensor)
            probabilities = torch.softmax(output, dim=1)
            confidence, predicted = torch.max(probabilities, 1)

            class_name = self.class_names[predicted.item()]
            conf = confidence.item()

        # Publish result
        result_msg = String()
        result_msg.data = f"{class_name} ({conf * 100:.1f}%)"
        self.result_pub.publish(result_msg)

        self.get_logger().info(f'Detected: {class_name} with confidence {conf:.2%}')

def main(args=None):
    rclpy.init(args=args)
    node = MLInferenceNode()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Practice Exercises

### Exercise 1: Object Classifier for Robot Manipulation

**Task:** Train a CNN to classify objects the robot will manipulate

**Requirements:**
- Collect dataset of 10 object classes (100 images each)
- Train ResNet-18 with transfer learning
- Achieve >90% validation accuracy
- Deploy as ROS 2 node
- Real-time inference (&lt;50ms per image)

### Exercise 2: Grasp Quality Predictor

**Task:** Predict grasp success from sensor data

**Requirements:**
- Collect 1000+ grasp attempts with success labels
- Train MLP on grasp parameters
- Achieve >85% accuracy
- Integrate with motion planning
- Online learning from new attempts

### Exercise 3: Anomaly Detection for Fault Diagnosis

**Task:** Detect anomalous robot behavior using unsupervised learning

**Requirements:**
- Collect normal operation data (joint torques, speeds, temps)
- Train autoencoder for anomaly detection
- Detect faults with &lt;5% false positive rate
- Real-time monitoring at 100 Hz
- Alert operator when anomaly detected

## Best Practices

### 1. Data Quality

- **Diverse data**: Cover full range of operating conditions
- **Balanced classes**: Equal samples per class (or use weighted loss)
- **Quality over quantity**: Clean, accurately labeled data beats large noisy datasets
- **Real robot data**: Simulated data doesn't always transfer

### 2. Model Selection

- **Start simple**: Try linear models before deep learning
- **Transfer learning**: Use pre-trained models when possible
- **Match capacity to data**: Don't use huge models with small datasets
- **Consider inference time**: Real-time requirements constrain model size

### 3. Training

- **Data augmentation**: Especially important for small datasets
- **Regularization**: Dropout, weight decay, early stopping
- **Monitor validation**: Avoid overfitting
- **Reproducibility**: Set random seeds, save configs

### 4. Deployment

- **Optimize for speed**: Pruning, quantization, batching
- **Test on target hardware**: Laptop GPUs ≠ embedded devices
- **Graceful degradation**: Handle edge cases safely
- **Monitor in production**: Log failures for retraining

## Common Pitfalls

1. **Overfitting small datasets**: Use regularization and augmentation
2. **Class imbalance**: Use weighted loss or resampling
3. **Data leakage**: Ensure train/val/test split is clean
4. **Ignoring inference time**: Model too slow for real-time control
5. **Not testing on real data**: Sim-to-real gap can be large

## Further Reading

**Books:**
- "Deep Learning" by Goodfellow, Bengio, Courville
- "Hands-On Machine Learning" by Géron
- "Deep Learning for Coders" by Howard & Gugger

**Online Courses:**
- [PyTorch Tutorials](https://pytorch.org/tutorials/)
- [Fast.ai Practical Deep Learning](https://course.fast.ai/)
- [Stanford CS231n](http://cs231n.stanford.edu/)

**Papers:**
- "ImageNet Classification with Deep CNNs" (Krizhevsky et al., 2012)
- "Batch Normalization" (Ioffe & Szegedy, 2015)
- "Adam Optimizer" (Kingma & Ba, 2014)

## Next Steps

Now that you understand ML fundamentals:

1. **Chapter 16**: Deep dive into reinforcement learning for robot control
2. **Chapter 17**: Use large language models for robot task planning
3. **Chapter 18**: Learn vision-language-action models for manipulation
4. **Project 4**: Build ML-powered vision system for manipulation

:::tip Key Takeaway
Machine learning enables robots to handle real-world complexity, but success requires careful attention to data quality, model selection, and deployment constraints. Start with simple models, iterate quickly, and always validate on real robot hardware.
:::
