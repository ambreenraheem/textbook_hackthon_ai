---
id: ch31-safety-ethics
title: Chapter 31 - Safety & Ethics
description: Address robot safety standards, ethical considerations, and responsible AI practices.
sidebar_label: Ch 31. Safety & Ethics
sidebar_position: 3
keywords: [safety, ethics, responsible AI, standards, regulations]
difficulty: intermediate
estimatedReadingTime: 25
---

# Chapter 31: Safety & Ethics

## Learning Objectives

By the end of this chapter, you will be able to:
- Understand robot safety standards and regulations
- Address ethical considerations in robotics
- Implement responsible AI practices
- Handle privacy and security concerns

## Introduction

As humanoid robots transition from research labs to homes, workplaces, and public spaces, **safety and ethics** become paramount. A robot that can walk, manipulate objects, and interact with humans poses unique risks—from physical harm due to mechanical failures to societal impacts from biased AI algorithms.

This chapter addresses:
1. **Robot Safety Standards**: International regulations and engineering practices (ISO 13482, ISO/TS 15066)
2. **Ethical Frameworks**: Responsible AI, fairness, transparency, and accountability
3. **Privacy & Security**: Data protection and cybersecurity for connected robots
4. **Regulatory Compliance**: Navigating legal frameworks across jurisdictions

**Why This Matters**: A single safety incident can erode public trust and halt deployment. Ethical failures can perpetuate biases or violate privacy at scale. Proactive safety and ethical design is not optional—it's foundational to sustainable robotics.

---

## 1. Robot Safety Standards

### 1.1 International Safety Standards

#### ISO 13482: Safety Requirements for Personal Care Robots
- **Scope**: Covers mobile servant robots, physical assistant robots, and person carrier robots
- **Key Requirements**:
  - Risk assessment and mitigation (ISO 12100)
  - Protective measures (physical barriers, software limits)
  - Validation and verification testing
  - User information and training

**Example Application to Humanoid Robots**:
```yaml
Risk Assessment (ISO 13482 Compliant):
  Hazard: Unintended collision during locomotion
  Risk Level: High (potential injury)
  Mitigations:
    - LiDAR-based obstacle detection (redundant sensors)
    - Emergency stop accessible to users
    - Maximum velocity limits (0.5 m/s in crowded areas)
    - Compliant materials on contact surfaces
  Residual Risk: Medium (acceptable with user training)
```

#### ISO/TS 15066: Collaborative Robots (Cobots)
- **Power and Force Limiting**: Biomechanical limits for human-robot contact
- **Safety-Rated Monitored Stop**: Robot halts when human enters workspace
- **Hand Guiding**: Manual manipulation with force/torque sensing

**Biomechanical Limits (ISO/TS 15066 Annex A)**:
| Body Region       | Pressure Limit (N/cm²) | Force Limit (N) |
|-------------------|------------------------|-----------------|
| Skull/Forehead    | 110                    | 130             |
| Face              | 75                     | 65              |
| Neck              | 110                    | 150             |
| Chest             | 110                    | 140             |
| Abdomen           | 110                    | 110             |
| Hands/Fingers     | 140                    | 160             |

**ROS 2 Implementation**:
```python
# Force-limited manipulation node
import rclpy
from geometry_msgs.msg import WrenchStamped
from std_srvs.srv import Trigger

class SafetyMonitor(Node):
    def __init__(self):
        super().__init__('safety_monitor')
        self.force_sub = self.create_subscription(
            WrenchStamped, '/wrist_force', self.force_callback, 10)
        self.estop_client = self.create_client(Trigger, '/emergency_stop')

        # ISO/TS 15066 limits
        self.max_force = 140.0  # Newtons (chest region)
        self.max_pressure = 110.0  # N/cm²

    def force_callback(self, msg):
        force_magnitude = (msg.wrench.force.x**2 +
                          msg.wrench.force.y**2 +
                          msg.wrench.force.z**2)**0.5

        if force_magnitude > self.max_force:
            self.get_logger().error(f'Force limit exceeded: {force_magnitude:.1f}N')
            self.trigger_emergency_stop()

    def trigger_emergency_stop(self):
        req = Trigger.Request()
        self.estop_client.call_async(req)
```

### 1.2 Functional Safety (IEC 61508)

**Safety Integrity Levels (SIL)**:
- **SIL 1**: Low risk (probability of failure: 10⁻² to 10⁻³ per year)
- **SIL 2**: Medium risk (10⁻³ to 10⁻⁴)
- **SIL 3**: High risk (10⁻⁴ to 10⁻⁵) ← **Typical for humanoids**
- **SIL 4**: Very high risk (10⁻⁵ to 10⁻⁶)

**Safety Architecture Pattern**:
```
┌─────────────────────────────────────────┐
│  Application Layer (Non-Safety)        │
│  - Navigation, Manipulation, HRI        │
└──────────────┬──────────────────────────┘
               │ Safety API
┌──────────────▼──────────────────────────┐
│  Safety Layer (SIL 3 Certified)         │
│  - Joint limit monitoring               │
│  - Collision detection                  │
│  - Emergency stop logic                 │
│  - Watchdog timers                      │
└──────────────┬──────────────────────────┘
               │
┌──────────────▼──────────────────────────┐
│  Hardware Safety (Fail-Safe)            │
│  - Redundant sensors                    │
│  - Independent E-stop circuit           │
│  - Mechanical joint stops               │
└─────────────────────────────────────────┘
```

### 1.3 Practical Safety Design

#### Emergency Stop Systems
**Requirements**:
- **Hardware E-stop**: Independent of software (IEC 60204-1)
- **Accessibility**: Reachable from all interaction points
- **Fail-safe**: Maintained by spring force, released by power

**ROS 2 Implementation**:
```python
# Multi-layered emergency stop
class EmergencyStopManager(Node):
    def __init__(self):
        super().__init__('estop_manager')

        # Hardware E-stop via GPIO
        self.hw_estop_sub = self.create_subscription(
            Bool, '/hw_estop_status', self.hw_estop_callback, 10)

        # Software E-stop service
        self.sw_estop_srv = self.create_service(
            Trigger, '/emergency_stop', self.sw_estop_callback)

        # Controller state publisher
        self.state_pub = self.create_publisher(
            RobotState, '/robot_state', 10)

        self.is_stopped = False

    def hw_estop_callback(self, msg):
        """Hardware E-stop has priority"""
        if msg.data:  # E-stop pressed
            self.activate_estop('HARDWARE')

    def sw_estop_callback(self, request, response):
        """Software E-stop via service call"""
        self.activate_estop('SOFTWARE')
        response.success = True
        response.message = 'Emergency stop activated'
        return response

    def activate_estop(self, source):
        if not self.is_stopped:
            self.get_logger().critical(f'EMERGENCY STOP: {source}')
            self.is_stopped = True

            # Broadcast stop to all controllers
            state_msg = RobotState()
            state_msg.mode = RobotState.EMERGENCY_STOP
            state_msg.timestamp = self.get_clock().now().to_msg()
            self.state_pub.publish(state_msg)

            # Zero all joint commands
            self.stop_all_joints()
```

#### Collision Avoidance

**Layered Defense**:
1. **Planning Layer**: Pre-collision trajectory optimization
2. **Reactive Layer**: Real-time obstacle detection and stopping
3. **Physical Layer**: Compliant materials, rounded edges

**Example: Whole-Body Collision Avoidance**:
```python
from moveit_msgs.msg import CollisionObject
from shape_msgs.msg import SolidPrimitive

class CollisionMonitor(Node):
    def __init__(self):
        super().__init__('collision_monitor')

        # Subscribe to depth camera
        self.depth_sub = self.create_subscription(
            PointCloud2, '/camera/depth/points', self.depth_callback, 10)

        # Publish to MoveIt planning scene
        self.scene_pub = self.create_publisher(
            CollisionObject, '/collision_object', 10)

        self.safety_margin = 0.15  # meters

    def depth_callback(self, cloud):
        """Add dynamic obstacles to planning scene"""
        # Convert point cloud to voxel grid
        obstacles = self.voxelize_point_cloud(cloud)

        for i, obstacle in enumerate(obstacles):
            collision_obj = CollisionObject()
            collision_obj.header.frame_id = 'base_link'
            collision_obj.id = f'dynamic_obstacle_{i}'
            collision_obj.operation = CollisionObject.ADD

            # Define primitive shape
            primitive = SolidPrimitive()
            primitive.type = SolidPrimitive.BOX
            primitive.dimensions = [0.1, 0.1, 0.1]  # 10cm cube

            collision_obj.primitives.append(primitive)
            collision_obj.primitive_poses.append(obstacle.pose)

            self.scene_pub.publish(collision_obj)
```

---

## 2. Ethical Frameworks for Robotics

### 2.1 AI Ethics Principles

#### IEEE Ethically Aligned Design
**Core Principles**:
1. **Human Rights**: Respect dignity, privacy, and autonomy
2. **Well-being**: Prioritize safety and minimize harm
3. **Data Agency**: Users control their data
4. **Transparency**: Explainable AI decisions
5. **Accountability**: Clear responsibility for failures

#### Asimov's Laws (Updated for 2025)
Traditional Asimov's Laws are **insufficient** for modern AI:

**Limitations**:
- Ambiguous definitions ("harm," "human")
- No mechanism for conflict resolution
- Assumes perfect world knowledge

**Modern Interpretation**:
```yaml
Principle 1 - Do No Harm:
  Description: Minimize physical, psychological, and social harm
  Implementation:
    - Risk assessment before deployment
    - Continuous safety monitoring
    - Graceful degradation on failures

Principle 2 - Respect Autonomy:
  Description: Support human decision-making, not replace it
  Implementation:
    - Human-in-the-loop for critical decisions
    - Opt-out mechanisms
    - Transparent capabilities and limitations

Principle 3 - Fairness:
  Description: Equitable treatment across demographics
  Implementation:
    - Bias testing on diverse datasets
    - Accessibility for disabilities
    - Auditable decision-making

Principle 4 - Accountability:
  Description: Clear ownership of decisions and failures
  Implementation:
    - Logging all robot actions
    - Fault traceability
    - Liability frameworks
```

### 2.2 Bias and Fairness

#### Sources of Bias in Robotics
1. **Training Data Bias**: Underrepresentation of demographics
2. **Sensor Bias**: Cameras fail on dark skin (see Joy Buolamwini's research)
3. **Design Bias**: Anthropomorphic robots reflect designer demographics
4. **Interaction Bias**: Reinforcement learning from biased human feedback

**Example: Face Detection Bias**:
```python
import cv2
import numpy as np

class FairFaceDetector(Node):
    def __init__(self):
        super().__init__('fair_face_detector')

        # Load multiple detection models for robustness
        self.detectors = [
            cv2.CascadeClassifier('haarcascade_frontalface_default.xml'),
            # Add diverse training sets (e.g., Balanced Faces in the Wild)
        ]

    def detect_faces(self, image):
        """Ensemble detection to reduce bias"""
        all_detections = []

        for detector in self.detectors:
            faces = detector.detectMultiScale(image, scaleFactor=1.1, minNeighbors=5)
            all_detections.extend(faces)

        # Merge overlapping detections
        merged = self.non_max_suppression(all_detections)

        # Log detection statistics for bias monitoring
        self.log_detection_stats(merged)

        return merged

    def log_detection_stats(self, detections):
        """Track detection rates for bias auditing"""
        stats = {
            'timestamp': self.get_clock().now(),
            'num_faces': len(detections),
            'lighting_condition': self.estimate_lighting(),
        }
        # Store for periodic bias analysis
```

#### Fairness Metrics
- **Demographic Parity**: Equal detection rates across groups
- **Equalized Odds**: Equal true positive and false positive rates
- **Calibration**: Confidence scores reflect actual accuracy

**Bias Testing Checklist**:
- [ ] Test on diverse skin tones (Fitzpatrick scale I-VI)
- [ ] Evaluate across ages (children, elderly)
- [ ] Assess gender presentation variations
- [ ] Include people with disabilities (wheelchairs, prosthetics)
- [ ] Test in varied lighting conditions

### 2.3 Transparency and Explainability

**Why Explainability Matters**:
- **Trust**: Users accept decisions they understand
- **Debugging**: Identify failure modes
- **Compliance**: GDPR "right to explanation"

**Levels of Explanation**:
1. **Global**: How does the model work in general?
2. **Local**: Why did it make this specific decision?
3. **Counterfactual**: What would change the decision?

**Example: Explainable Grasping**:
```python
class ExplainableGraspPlanner(Node):
    def __init__(self):
        super().__init__('explainable_grasp_planner')
        self.explanation_pub = self.create_publisher(
            String, '/grasp_explanation', 10)

    def select_grasp(self, grasp_candidates):
        """Select grasp with explanation"""
        scores = [self.score_grasp(g) for g in grasp_candidates]
        best_idx = np.argmax(scores)
        best_grasp = grasp_candidates[best_idx]

        # Generate explanation
        explanation = self.explain_grasp_choice(best_grasp, grasp_candidates)
        self.explanation_pub.publish(String(data=explanation))

        return best_grasp

    def explain_grasp_choice(self, selected, alternatives):
        """Human-readable explanation"""
        explanation = f"Selected grasp with score {self.score_grasp(selected):.2f}\n"
        explanation += f"Reasons:\n"
        explanation += f"  - Force closure: {selected.force_closure_quality:.2f}\n"
        explanation += f"  - Approach clearance: {selected.clearance:.2f}m\n"
        explanation += f"  - Finger configuration: {selected.finger_config}\n"
        explanation += f"\nRejected {len(alternatives)-1} alternatives due to:\n"

        for i, alt in enumerate(alternatives[:3]):  # Top 3 rejected
            if alt is selected:
                continue
            reason = self.get_rejection_reason(alt, selected)
            explanation += f"  {i+1}. {reason}\n"

        return explanation
```

---

## 3. Privacy and Security

### 3.1 Privacy-by-Design

**Principles (GDPR Article 25)**:
1. **Data Minimization**: Collect only necessary data
2. **Purpose Limitation**: Use data only for stated purposes
3. **Storage Limitation**: Delete data when no longer needed
4. **Consent**: Explicit user permission

**Example: Privacy-Preserving HRI**:
```python
class PrivacyAwareHRI(Node):
    def __init__(self):
        super().__init__('privacy_hri')

        # Configuration
        self.consent_manager = ConsentManager()
        self.data_retention_days = 7

    def process_camera_feed(self, image):
        """Privacy-preserving vision processing"""

        # 1. Check consent
        if not self.consent_manager.has_consent('camera_processing'):
            self.get_logger().warn('No camera consent - skipping frame')
            return

        # 2. On-device processing (avoid cloud uploads)
        faces = self.detect_faces_locally(image)

        # 3. Anonymization: Face blurring for logs
        anonymized_image = self.blur_faces(image, faces)

        # 4. Feature extraction only (discard raw images)
        embeddings = self.extract_face_embeddings(faces)

        # 5. Differential privacy for aggregates
        num_people = len(faces) + np.random.laplace(0, 1.0)  # Add noise

        # 6. Encrypted storage
        self.store_encrypted(embeddings, expire_after=self.data_retention_days)

        return num_people  # Return only aggregate, not identifiable data
```

### 3.2 Cybersecurity for Robots

**Threat Model**:
- **Unauthorized Access**: Attackers control robot remotely
- **Data Exfiltration**: Stealing sensor data or learned models
- **Denial of Service**: Disrupting critical operations
- **Physical Harm**: Malicious commands causing injury

**Security Best Practices**:

#### 1. Secure Communication
```python
# Use DDS security (ROS 2)
# Enable authentication, encryption, and access control

# sros2_security_config.xml
<security>
    <authentication>
        <identity_ca>file://path/to/ca.cert.pem</identity_ca>
        <identity_certificate>file://path/to/cert.pem</identity_certificate>
        <private_key>file://path/to/key.pem</private_key>
    </authentication>
    <access_control>
        <governance>file://path/to/governance.xml</governance>
        <permissions>file://path/to/permissions.xml</permissions>
    </access_control>
</security>
```

#### 2. Input Validation
```python
class SecureJointController(Node):
    def __init__(self):
        super().__init__('secure_joint_controller')
        self.cmd_sub = self.create_subscription(
            JointTrajectory, '/joint_commands', self.command_callback, 10)

        # Safety limits
        self.joint_limits = {
            'shoulder_pitch': (-2.0, 2.0),  # radians
            'elbow': (-2.5, 0.0),
            'wrist': (-1.5, 1.5),
        }

    def command_callback(self, msg):
        """Validate all commands before execution"""
        if not self.validate_command(msg):
            self.get_logger().error('Invalid command rejected')
            return

        # Execute validated command
        self.execute_trajectory(msg)

    def validate_command(self, trajectory):
        """Security-critical validation"""
        for point in trajectory.points:
            for i, joint_name in enumerate(trajectory.joint_names):
                position = point.positions[i]
                min_pos, max_pos = self.joint_limits[joint_name]

                # Check limits
                if not (min_pos <= position <= max_pos):
                    self.get_logger().error(
                        f'Joint {joint_name} command {position} '
                        f'outside limits [{min_pos}, {max_pos}]')
                    return False

                # Check velocity
                if abs(point.velocities[i]) > 2.0:  # rad/s
                    self.get_logger().error('Excessive velocity command')
                    return False

        return True
```

#### 3. Anomaly Detection
```python
class SecurityMonitor(Node):
    def __init__(self):
        super().__init__('security_monitor')

        # Baseline behavior model
        self.behavior_model = self.load_baseline_behavior()

        # Monitor all topics
        self.create_subscription(AnyMsg, '/.*', self.monitor_callback, 10,
                                 regex=True)

    def monitor_callback(self, msg):
        """Detect anomalous robot behavior"""
        features = self.extract_features(msg)
        anomaly_score = self.behavior_model.score(features)

        if anomaly_score > self.threshold:
            self.get_logger().critical(
                f'SECURITY ALERT: Anomalous behavior detected '
                f'(score: {anomaly_score:.2f})')
            self.trigger_security_response()

    def trigger_security_response(self):
        """Automated incident response"""
        # 1. Log incident
        self.log_security_incident()

        # 2. Reduce capabilities (graceful degradation)
        self.limit_robot_actions()

        # 3. Notify administrators
        self.send_alert()
```

---

## 4. Regulatory Compliance

### 4.1 Global Regulatory Landscape

| Region | Key Regulations | Focus Areas |
|--------|----------------|-------------|
| **EU** | AI Act, GDPR, Machinery Directive | High-risk AI, data privacy |
| **USA** | NIST AI RMF, FTC Act | Algorithmic fairness, consumer protection |
| **Japan** | Robot Strategy 2020 | Service robots, harmonized standards |
| **China** | Personal Information Protection Law | Data localization, algorithmic recommendations |

#### EU AI Act (2024)
**Risk Categories**:
- **Unacceptable Risk**: Banned (e.g., social scoring)
- **High Risk**: Strict requirements (e.g., humanoid care robots)
  - Conformity assessments
  - Risk management systems
  - Human oversight
  - Transparency obligations
- **Limited Risk**: Transparency (e.g., chatbots)
- **Minimal Risk**: No restrictions

**Compliance for Humanoid Robots (High Risk)**:
```yaml
AI Act Compliance Checklist:
  Risk Management:
    - [ ] Documented risk assessment (ISO 14971)
    - [ ] Post-market monitoring plan
    - [ ] Incident reporting procedure

  Data Governance:
    - [ ] Training data documentation
    - [ ] Bias testing results
    - [ ] Data provenance tracking

  Technical Documentation:
    - [ ] System architecture description
    - [ ] Validation and testing reports
    - [ ] Cybersecurity measures

  Human Oversight:
    - [ ] Emergency stop accessible
    - [ ] Override mechanisms
    - [ ] Operator training materials

  Transparency:
    - [ ] User instructions (capabilities, limitations)
    - [ ] Automated decision disclosure
    - [ ] Contact for questions
```

### 4.2 Liability and Insurance

**Who's Liable?**
- **Manufacturer**: Design defects, inadequate warnings
- **Operator**: Misuse, inadequate maintenance
- **Software Developer**: Algorithmic failures, security vulnerabilities
- **AI System**: Emerging "electronic personhood" debates

**Insurance Considerations**:
- **Product Liability**: Covers manufacturing defects
- **Cyber Insurance**: Data breaches, ransomware
- **Professional Liability**: Errors in AI decisions
- **Robot-Specific Policies**: Emerging market (e.g., Munich Re)

---

## 5. Responsible AI in Practice

### 5.1 Development Lifecycle

**Stage 1: Design**
- Stakeholder consultation (users, ethicists, domain experts)
- Value-sensitive design workshops
- Participatory design with end users

**Stage 2: Development**
- Diverse development teams (reduce design bias)
- Bias audits at each milestone
- Open-source components for transparency

**Stage 3: Testing**
- Red team exercises (adversarial testing)
- Diverse user testing (accessibility, cultural contexts)
- Long-term field trials

**Stage 4: Deployment**
- Gradual rollout with monitoring
- Feedback mechanisms for users
- A/B testing for fairness

**Stage 5: Monitoring**
- Real-time bias detection
- Incident response protocols
- Regular audits (quarterly recommended)

### 5.2 Organizational Practices

**Ethics Review Boards**:
- Composition: Engineers, ethicists, legal, user representatives
- Mandate: Review high-risk deployments
- Example: Google's Advanced Technology External Advisory Council

**Responsible AI Checklists**:
```markdown
Pre-Deployment Checklist:
- [ ] Safety testing completed (10,000+ hours recommended)
- [ ] Bias audit passed on diverse test sets
- [ ] Privacy impact assessment approved
- [ ] Security penetration testing passed
- [ ] Regulatory compliance verified
- [ ] User documentation finalized
- [ ] Incident response plan in place
- [ ] Insurance coverage confirmed
- [ ] Ethics board approval obtained
```

---

## 6. Case Studies

### Case Study 1: Autonomous Delivery Robots (Starship Technologies)
**Challenge**: Sidewalk navigation with pedestrians
**Safety Measures**:
- Maximum speed: 6 km/h (walking pace)
- 9 cameras with 360° vision
- Remote operator monitoring (1:100 ratio)
- Locked cargo compartment

**Outcome**: 5+ million deliveries, near-zero injury incidents

### Case Study 2: Collaborative Robot Bias (Industrial Cobots)
**Challenge**: Hand-guiding systems failed on darker skin tones
**Root Cause**: Capacitive sensors calibrated for light skin only
**Solution**: Multi-modal sensing (vision + capacitive + force/torque)
**Lesson**: Test sensors across diverse user populations

### Case Study 3: Privacy Violation (Robot Vacuum)
**Incident**: iRobot vacuum photos leaked online (2022)
**Cause**: Development units with debug mode shared training data
**Impact**: Trust erosion, regulatory scrutiny
**Prevention**: Disable data collection in production devices, encrypt all transmissions

---

## 7. Future Challenges

### 7.1 Emerging Ethical Dilemmas

**Autonomous Moral Decisions**:
- Trolley problem for mobile robots (harm minimization)
- Resource allocation (who gets robot assistance first?)
- Consent in vulnerable populations (children, elderly)

**AI Rights and Personhood**:
- Should advanced AI have legal status?
- Ownership of AI-generated work
- "Right to be forgotten" for learned robot behaviors

### 7.2 Long-Term Considerations

**Job Displacement**:
- 85 million jobs displaced by 2025 (World Economic Forum)
- Need for retraining programs and social safety nets

**Human-Robot Relationships**:
- Emotional attachment to robots (especially in caregiving)
- Deception and manipulation risks
- Societal impacts on human relationships

**Environmental Impact**:
- Energy consumption of large-scale robot fleets
- E-waste from obsolete robots
- Sustainable materials and circular economy

---

## Summary

This chapter covered:
1. **Safety Standards**: ISO 13482, ISO/TS 15066, functional safety (IEC 61508)
2. **Ethical Frameworks**: AI ethics principles, bias mitigation, explainability
3. **Privacy & Security**: GDPR compliance, cybersecurity best practices
4. **Regulatory Compliance**: EU AI Act, liability, insurance
5. **Responsible AI**: Development lifecycle, organizational practices
6. **Case Studies**: Real-world safety and ethical lessons

**Key Takeaways**:
- Safety and ethics are **engineering requirements**, not afterthoughts
- **Proactive** bias testing and mitigation are essential
- **Transparency** builds trust and enables debugging
- **Regulatory compliance** is increasingly mandatory for deployment
- **Continuous monitoring** is needed throughout the robot lifecycle

---

## Exercises

### Exercise 1: Risk Assessment
Conduct an ISO 13482 risk assessment for a humanoid home assistant robot. Identify:
1. Top 5 hazards (e.g., collision, electrical, pinch points)
2. Risk levels (probability × severity)
3. Mitigation strategies
4. Residual risks

### Exercise 2: Bias Testing
Implement a bias testing pipeline for a face detection system:
```python
# Test face detection across demographics
from sklearn.metrics import confusion_matrix

def test_bias(detector, test_dataset):
    """
    test_dataset: labeled images with demographic annotations
    Returns: Fairness metrics by group
    """
    results = {}
    for group in ['skin_tone_1-2', 'skin_tone_3-4', 'skin_tone_5-6']:
        group_data = test_dataset.filter(skin_tone=group)
        y_true, y_pred = evaluate_detector(detector, group_data)

        # Compute fairness metrics
        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
        results[group] = {
            'true_positive_rate': tp / (tp + fn),
            'false_positive_rate': fp / (fp + tn),
            'accuracy': (tp + tn) / (tp + tn + fp + fn)
        }

    # Check for demographic parity
    check_demographic_parity(results)
    return results
```

### Exercise 3: Privacy Impact Assessment
Draft a privacy impact assessment for a home robot with:
- Camera and microphone sensors
- Cloud-based speech recognition
- Learned user preferences

Address:
1. Data flows (collection → processing → storage → deletion)
2. Consent mechanisms
3. Anonymization techniques
4. Data breach response plan

### Exercise 4: Security Hardening
Implement these security features for a ROS 2 robot:
1. Enable DDS security with authentication and encryption
2. Input validation for all command topics
3. Anomaly detection for unusual movement patterns
4. Secure logging of all actions for audit trails

---

## Further Reading

**Standards and Regulations**:
- [ISO 13482:2014 - Robots and robotic devices](https://www.iso.org/standard/53820.html)
- [EU AI Act - Full Text](https://artificialintelligenceact.eu/)
- [NIST AI Risk Management Framework](https://www.nist.gov/itl/ai-risk-management-framework)

**Ethics Research**:
- Kate Crawford: *Atlas of AI* (2021)
- Virginia Dignum: *Responsible Artificial Intelligence* (2019)
- Abeba Birhane: "Algorithmic Injustice" (research papers)

**Privacy & Security**:
- [ROS 2 Security Documentation](https://design.ros2.org/articles/ros2_security.html)
- [GDPR Compliance for IoT](https://gdpr.eu/iot/)
- Bruce Schneier: *Click Here to Kill Everybody* (2018)

**Case Studies**:
- [AI Incident Database](https://incidentdatabase.ai/)
- [Partnership on AI Case Studies](https://partnershiponai.org/)

---

## Next Chapter

In [Chapter 32: Future of Physical AI](/docs/part-09-advanced/ch32-future-physical-ai), we'll explore:
- Emerging trends in embodied AI
- Technical challenges on the horizon
- Career opportunities in robotics
- The future of human-robot collaboration
